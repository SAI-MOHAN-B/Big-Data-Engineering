{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbfe4c1-1efc-4058-945a-00726ff5370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3142ffa0-cac0-4ccb-ab26-92285d74d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb09ced7-f65f-4720-b29d-649a2dcfdc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").config(\"spark.sql.autoBroadcastJoinThreshold\",104857600).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82d9305f-cd9d-4dc4-a23c-13da010d7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\",True).load(\"employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7e8e2e-6e70-4496-bed0-e7790b270939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n",
      "|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n",
      "|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n",
      "|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n",
      "|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|            - |       102|           60|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|            - |       103|           60|\n",
      "|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|            - |       103|           60|\n",
      "|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|            - |       103|           60|\n",
      "|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|            - |       103|           60|\n",
      "|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|\n",
      "|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|\n",
      "|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b676bc68-23fd-4a00-aa54-4ab3f1ac5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(50,\"sales\"),(40,\"IT\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43a4f47a-70e1-410d-b91a-673e01fd47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\"DEPARTMENT_ID\",\"DEPT_NAME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c4621f-0a20-4ce6-8416-2244ec194cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(data=data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc839822-561e-468a-8bfa-feb0cd30c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [cast(DEPARTMENT_ID#127 as bigint)], [DEPARTMENT_ID#196L], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(DEPARTMENT_ID#127)\n",
      "   :  +- FileScan csv [EMPLOYEE_ID#117,FIRST_NAME#118,LAST_NAME#119,EMAIL#120,PHONE_NUMBER#121,HIRE_DATE#122,JOB_ID#123,SALARY#124,COMMISSION_PCT#125,MANAGER_ID#126,DEPARTMENT_ID#127] Batched: false, DataFilters: [isnotnull(DEPARTMENT_ID#127)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/fci/Desktop/Spark-Tutorials/employees.csv], PartitionFilters: [], PushedFilters: [IsNotNull(DEPARTMENT_ID)], ReadSchema: struct<EMPLOYEE_ID:int,FIRST_NAME:string,LAST_NAME:string,EMAIL:string,PHONE_NUMBER:string,HIRE_D...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=153]\n",
      "      +- Filter isnotnull(DEPARTMENT_ID#196L)\n",
      "         +- Scan ExistingRDD[DEPARTMENT_ID#196L,DEPT_NAME#197]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = df.join(broadcast(sales_df),df.DEPARTMENT_ID == sales_df.DEPARTMENT_ID,how=\"inner\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e61a5-97ab-4e84-988d-61cff3541c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # First, It read the parquet file and created a Larger DataFrame with limited records.\n",
    "    # Then, BroadcastHashJoin is performed between the smallerDF and LargerDF using the condition provided.\n",
    "    # Even if the smallerDF is not specified to be broadcasted in our code, Spark automatically broadcasts the smaller DataFrame into executor memory by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d4168-f8df-422c-be66-737950947032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc6251-8976-4ace-a7c8-bc3c58a8ae68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1664ba6-cea0-4e63-b5ce-1221a1e82662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecad02d-69b4-4a77-a496-6eee4bddb0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
