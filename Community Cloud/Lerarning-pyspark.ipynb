{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4608786-17e4-4a59-94e4-e8b2d2def03f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a274e2d-7aa2-4dc0-9dec-c4a9f37d17c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac4536ae-9071-47f6-926c-220dc271ddde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4569c2f0-878f-4c82-a318-a52f2a194d01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([12,13,14,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c354d34e-dc08-4d78-9992-16fa22f58ee6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "def add(a,b):\n",
    "    return a+b\n",
    "result = rdd.reduce(add)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697c2319-349f-49c6-a94c-f41dac909383",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2479340638257166#setting/sparkui/0604-093824-mgejv40z/driver-7338615096630975181\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=2479340638257166#setting/sparkui/0604-093824-mgejv40z/driver-7338615096630975181\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55b0d9b-61f8-43fa-af1d-c694ed9147c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1,[20.0,30.0,40.0])],(\"id\",\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1afe0769-5878-44e9-a345-0201fc641314",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n| id|            values|\n+---+------------------+\n|  1|[20.0, 30.0, 40.0]|\n+---+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bbfdf50-992f-4594-b09a-14b35d6852f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4365526877570898>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lit,reduce\n",
       "\n",
       "\u001B[0;31mImportError\u001B[0m: cannot import name 'reduce' from 'pyspark.sql.functions' (/databricks/spark/python/pyspark/sql/functions.py)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\nFile \u001B[0;32m<command-4365526877570898>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lit,reduce\n\n\u001B[0;31mImportError\u001B[0m: cannot import name 'reduce' from 'pyspark.sql.functions' (/databricks/spark/python/pyspark/sql/functions.py)",
       "errorSummary": "<span class='ansi-red-fg'>ImportError</span>: cannot import name 'reduce' from 'pyspark.sql.functions' (/databricks/spark/python/pyspark/sql/functions.py)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit,reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dffa27c-afa3-40c3-8566-ff7ff97c628c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4365526877570896>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m total_sum \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mreduce\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m,lit(\u001B[38;5;241m0.0\u001B[39m),\u001B[38;5;28;01mlambda\u001B[39;00m acc,x: acc\u001B[38;5;241m+\u001B[39mx)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'reduce' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4365526877570896>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m total_sum \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mreduce\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m,lit(\u001B[38;5;241m0.0\u001B[39m),\u001B[38;5;28;01mlambda\u001B[39;00m acc,x: acc\u001B[38;5;241m+\u001B[39mx)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\n\u001B[0;31mNameError\u001B[0m: name 'reduce' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'reduce' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_sum = df.select(reduce(\"values\",lit(0.0),lambda acc,x: acc+x).alias(\"sum\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aef0eb9-131b-407b-ade1-f0ea18d21588",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lerarning-pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
